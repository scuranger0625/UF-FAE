import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import unix_timestamp, concat_ws, col
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import (
    LogisticRegression, DecisionTreeClassifier,
    RandomForestClassifier, LinearSVC
)
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml import Pipeline

# === 1. å»ºç«‹ SparkSessionï¼ˆå–®ä¸€å…¥å£ï¼‰ ===
spark = SparkSession.builder.appName("SAML-D GraphOnly ML with TimeSeries Split").getOrCreate()
spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")  # é¿å… datetime è§£æéŒ¯èª¤

# === 2. è®€å–å¤šæ¨¡æ…‹ç‰¹å¾µ parquetï¼ˆæœ‰åœ–ç‰¹å¾µï¼‰ ===
df = spark.read.parquet("gs://saml-d/SAML-D_with_graph_centrality.parquet")

# === 3. åˆä½µ Date+Time æˆ timestampï¼ˆç…§èˆŠï¼Œåˆ‡æ™‚é–“ç”¨ï¼‰ ===
df = df.withColumn(
    "timestamp",
    unix_timestamp(concat_ws(" ", col("Date"), col("Time")), "yyyy-MM-dd HH:mm:ss").cast("long")
)

# === 4. åš´æ ¼æ™‚é–“åºåˆ—åˆ‡åˆ†ï¼ˆå‡å†ªæ’åºï¼Œ80%è¨“ç·´ã€20%æ¸¬è©¦ï¼‰===
df = df.orderBy("timestamp")
total = df.count()
split_idx = int(total * 0.8)
train_data = df.limit(split_idx)
test_data = df.subtract(train_data)

# === 5. åªå–åœ–ç‰¹å¾µä½œç‚º numeric features ===
graph_cols = [
    "group_node_count", "group_edge_count", "group_bidirect_ratio",
    "sender_degree", "receiver_degree",
    "sender_closeness", "receiver_closeness",
    "sender_betweenness", "receiver_betweenness"
]
# ã€ä¸åŠ å…¥ä»»ä½•åŸå§‹äº¤æ˜“ã€å¸³æˆ¶ã€é‡‘é¡ã€åœ°ç†ç­‰æ¬„ä½ã€‘

assembler = VectorAssembler(inputCols=graph_cols, outputCol="features")

# === 6. å®šç¾© ML æ¨¡å‹ ===
models = {
    "Logistic Regression": LogisticRegression(labelCol="Is_laundering", featuresCol="features"),
    "Decision Tree": DecisionTreeClassifier(labelCol="Is_laundering", featuresCol="features"),
    "Random Forest": RandomForestClassifier(labelCol="Is_laundering", featuresCol="features", numTrees=100),
    "SVM (LinearSVC)": LinearSVC(labelCol="Is_laundering", featuresCol="features")
}

# === 7. è¨­å®šè©•ä¼°æŒ‡æ¨™ ===
binary_eval = BinaryClassificationEvaluator(labelCol="Is_laundering", metricName="areaUnderROC")
precision_eval = MulticlassClassificationEvaluator(labelCol="Is_laundering", predictionCol="prediction", metricName="precisionByLabel")
recall_eval = MulticlassClassificationEvaluator(labelCol="Is_laundering", predictionCol="prediction", metricName="recallByLabel")
f1_eval = MulticlassClassificationEvaluator(labelCol="Is_laundering", predictionCol="prediction", metricName="f1")

# === 8. åŸ·è¡Œè¨“ç·´/é æ¸¬/é‡è¦æ€§åˆ†æ ===
for name, clf in models.items():
    print(f"ğŸ”¹ {name} (Graph-Only Features)")
    start = time.time()
    pipeline = Pipeline(stages=[assembler, clf])
    model = pipeline.fit(train_data)
    predictions = model.transform(test_data)
    elapsed = time.time() - start

    auc = binary_eval.evaluate(predictions)
    precision = precision_eval.evaluate(predictions)
    recall = recall_eval.evaluate(predictions)
    f1 = f1_eval.evaluate(predictions)

    print(f"   ğŸ•’ è¨“ç·´+é æ¸¬æ™‚é–“ï¼š{elapsed:.2f} ç§’")
    print(f"   ğŸ“ˆ AUC          ï¼š{auc:.4f}")
    print(f"   ğŸ¯ Precision    ï¼š{precision:.4f}")
    print(f"   ğŸ¯ Recall       ï¼š{recall:.4f}")
    print(f"   ğŸ§® F1 Score     ï¼š{f1:.4f}\n")

    # === ç‰¹å¾µé‡è¦æ€§åˆ†æ ===
    if name in ["Decision Tree", "Random Forest"]:
        importances = model.stages[-1].featureImportances
        feature_names = graph_cols
        imp_list = list(importances)
        sorted_features = sorted(zip(feature_names, imp_list), key=lambda x: -x[1])[:8]
        for fname, score in sorted_features:
            print(f"      {fname:<25} {score:.4f}")
        print()
    elif name == "Logistic Regression":
        coefs = model.stages[-1].coefficients.toArray()
        top_idx = abs(coefs).argsort()[-8:][::-1]
        for idx in top_idx:
            print(f"      {graph_cols[idx]:<25} abs(coef): {abs(coefs[idx]):.4f}")
        print()
    elif name == "SVM (LinearSVC)":
        coefs = model.stages[-1].coefficients.toArray()
        top_idx = abs(coefs).argsort()[-8:][::-1]
        for idx in top_idx:
            print(f"      {graph_cols[idx]:<25} abs(coef): {abs(coefs[idx]):.4f}")
        print()

print("âœ… åš´æ ¼æ™‚é–“åºåˆ—é©—è­‰ï¼ˆåƒ…ç”¨åœ–ç‰¹å¾µï¼‰ML pipeline åŸ·è¡Œå®Œç•¢ï¼")
