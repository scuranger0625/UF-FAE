import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import unix_timestamp, concat_ws, col
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml.classification import (
    LogisticRegression, DecisionTreeClassifier,
    RandomForestClassifier, LinearSVC
)
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml import Pipeline

# === 1. å»ºç«‹ SparkSessionï¼ˆå–®ä¸€å…¥å£ï¼‰ ===
spark = SparkSession.builder.appName("SAML-D Multimodal ML with TimeSeries Split").getOrCreate()
spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")  # æ™‚é–“æ ¼å¼å‘ä¸‹ç›¸å®¹ï¼Œé¿å… datetime è§£æéŒ¯èª¤

# === 2. è®€å–å¤šæ¨¡æ…‹ç‰¹å¾µèåˆçš„ parquetï¼ˆå·²ç¶“æœ‰åœ–ç‰¹å¾µèˆ‡åŸå§‹è³‡æ–™ï¼‰ ===
df = spark.read.parquet("gs://saml-d/SAML-D_with_graph_centrality.parquet")

# === 3. åˆä½µ Date+Time æˆ timestampï¼Œè½‰æˆ long æ ¼å¼å¯æ’åº ===
df = df.withColumn(
    "timestamp",
    unix_timestamp(concat_ws(" ", col("Date"), col("Time")), "yyyy-MM-dd HH:mm:ss").cast("long")
)

# === 4. åš´æ ¼ä¾æ™‚é–“åºåˆ—åˆ‡åˆ†ï¼ˆæŒ‰ timestamp å‡å†ªæ’åºï¼Œ80%è¨“ç·´ 20%æ¸¬è©¦ï¼‰===
df = df.orderBy("timestamp")      # ä¿è­‰è³‡æ–™æ˜¯ä¾äº¤æ˜“æ™‚é–“éå¢
total = df.count()                # è³‡æ–™ç¸½ç­†æ•¸
split_idx = int(total * 0.8)      # åˆ‡å‰²é»ï¼Œ80%çµ¦è¨“ç·´é›†
train_data = df.limit(split_idx)  # å–å‰ split_idx ç­†ç‚ºè¨“ç·´é›†
test_data = df.subtract(train_data)  # å‰©ä¸‹ 20% ç‚ºæ¸¬è©¦é›†ï¼ˆä¿è­‰åš´æ ¼æœªä¾†è³‡è¨Šä¸å¯è¦‹ï¼‰

# === 5. ç‰¹å¾µå·¥ç¨‹è¨­è¨ˆï¼šåˆ†é¡ç‰¹å¾µ + æ•¸å€¼ç‰¹å¾µ ===
categorical_cols = [
    "Payment_currency", "Received_currency",
    "Sender_bank_location", "Receiver_bank_location", "Payment_type"
]
numeric_cols = [
    "Amount",  # äº¤æ˜“é‡‘é¡
    # ä¸‹æ–¹çš†ç‚ºåœ–çµæ§‹ derived features
    "group_node_count", "group_edge_count", "group_bidirect_ratio",
    "sender_degree", "receiver_degree",
    "sender_closeness", "receiver_closeness",
    "sender_betweenness", "receiver_betweenness"
]

# === 6. è½‰æ›é¡åˆ¥å‹ç‰¹å¾µç‚ºæ•¸å­—å‘é‡ï¼ˆå…ˆç·¨ç¢¼å† OneHotï¼‰===
indexers = [StringIndexer(inputCol=c, outputCol=f"{c}_idx", handleInvalid="keep") for c in categorical_cols]
encoders = [OneHotEncoder(inputCol=f"{c}_idx", outputCol=f"{c}_vec") for c in categorical_cols]

# æ•¸å€¼+OneHotç·¨ç¢¼åˆä½µç‚º features å‘é‡
feature_cols = numeric_cols + [f"{c}_vec" for c in categorical_cols]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

# === 7. å®šç¾©å¤šå€‹ç¶“å…¸ ML æ¨¡å‹ ===
models = {
    "Logistic Regression": LogisticRegression(labelCol="Is_laundering", featuresCol="features"),
    "Decision Tree": DecisionTreeClassifier(labelCol="Is_laundering", featuresCol="features"),
    "Random Forest": RandomForestClassifier(labelCol="Is_laundering", featuresCol="features", numTrees=100),
    "SVM (LinearSVC)": LinearSVC(labelCol="Is_laundering", featuresCol="features")
}

# === 8. è¨­å®šå„ç¨®å¸¸ç”¨åˆ†é¡è©•ä¼°æŒ‡æ¨™ï¼ˆAUCã€Precisionã€Recallã€F1ï¼‰===
binary_eval = BinaryClassificationEvaluator(labelCol="Is_laundering", metricName="areaUnderROC")
precision_eval = MulticlassClassificationEvaluator(labelCol="Is_laundering", predictionCol="prediction", metricName="precisionByLabel")
recall_eval = MulticlassClassificationEvaluator(labelCol="Is_laundering", predictionCol="prediction", metricName="recallByLabel")
f1_eval = MulticlassClassificationEvaluator(labelCol="Is_laundering", predictionCol="prediction", metricName="f1")

# === 9. ä¾åºè¨“ç·´æ¯å€‹æ¨¡å‹ï¼Œè¼¸å‡ºè¨“ç·´æ™‚é–“ã€å„æŒ‡æ¨™ã€ç‰¹å¾µé‡è¦æ€§ï¼ˆTop 10ï¼‰ ===
for name, clf in models.items():
    print(f"ğŸ”¹ {name}")
    start = time.time()
    pipeline = Pipeline(stages=indexers + encoders + [assembler, clf])
    model = pipeline.fit(train_data)       # æ¨¡å‹è¨“ç·´ï¼ˆåƒ…ç”¨éå»è³‡æ–™ï¼‰
    predictions = model.transform(test_data)  # æ¸¬è©¦é›†é æ¸¬ï¼ˆæœªä¾†è³‡æ–™ï¼‰
    elapsed = time.time() - start

    # === 9.1 è¼¸å‡ºå„æŒ‡æ¨™ ===
    auc = binary_eval.evaluate(predictions)
    precision = precision_eval.evaluate(predictions)
    recall = recall_eval.evaluate(predictions)
    f1 = f1_eval.evaluate(predictions)

    print(f"   ğŸ•’ è¨“ç·´+é æ¸¬æ™‚é–“ï¼š{elapsed:.2f} ç§’")
    print(f"   ğŸ“ˆ AUC          ï¼š{auc:.4f}")
    print(f"   ğŸ¯ Precision    ï¼š{precision:.4f}")
    print(f"   ğŸ¯ Recall       ï¼š{recall:.4f}")
    print(f"   ğŸ§® F1 Score     ï¼š{f1:.4f}\n")

    # === 9.2 ç‰¹å¾µé‡è¦æ€§ï¼ˆåƒ… DT/RF æœ‰ .featureImportancesï¼‰===
    if name in ["Decision Tree", "Random Forest"]:
        importances = model.stages[-1].featureImportances
        print("   ğŸ”¬ Feature Importances:")
        feature_names = numeric_cols.copy()
        # æ ¹æ“šé¡åˆ¥æ•¸é‡æ±ºå®š OneHot è¼¸å‡ºé•·åº¦ï¼ˆç°¡åŒ–è¨­ 20ï¼Œå¯¦å‹™å¯ç”¨å­—å…¸å‹•æ…‹æŸ¥ï¼‰
        for c in categorical_cols:
            ohe_size = 20
            feature_names.extend([f"{c}_vec_{i}" for i in range(ohe_size)])
        imp_list = list(importances)
        sorted_features = sorted(zip(feature_names, imp_list), key=lambda x: -x[1])[:10]
        for fname, score in sorted_features:
            print(f"      {fname:<25} {score:.4f}")
        print()
    # === 9.3 ç·šæ€§æ¨¡å‹ï¼ˆLogistic/SVMï¼‰å‰‡å°å‰10å¤§çµ•å°ä¿‚æ•¸ ===
    elif name == "Logistic Regression":
        coefs = model.stages[-1].coefficients.toArray()
        top_idx = abs(coefs).argsort()[-10:][::-1]
        for idx in top_idx:
            print(f"      feature_{idx:<2} abs(coef): {abs(coefs[idx]):.4f}")
        print()
    elif name == "SVM (LinearSVC)":
        coefs = model.stages[-1].coefficients.toArray()
        top_idx = abs(coefs).argsort()[-10:][::-1]
        for idx in top_idx:
            print(f"      feature_{idx:<2} abs(coef): {abs(coefs[idx]):.4f}")
        print()

print("âœ… åš´æ ¼æ™‚é–“åºåˆ—é©—è­‰èˆ‡å¤šæ¨¡æ…‹ç‰¹å¾µèåˆ ML pipeline åŸ·è¡Œå®Œç•¢ï¼")
