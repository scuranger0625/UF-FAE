import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import unix_timestamp, concat_ws, col
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml.classification import (
    LogisticRegression, DecisionTreeClassifier,
    RandomForestClassifier, LinearSVC
)
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml import Pipeline

# === 1. å»ºç«‹ SparkSession ===
spark = SparkSession.builder.appName("SAML-D Baseline ML with TimeSeries Split").getOrCreate()
spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")  # é¿å… datetime è§£æéŒ¯èª¤

# === 2. è®€å– Parquet åŸå§‹è³‡æ–™ ===
df = spark.read.parquet("gs://saml-d/SAML-D.parquet")  # æ³¨æ„ï¼é€™è£¡ä¸è¦ç”¨_with_graph_centrality

# === 3. åˆä½µæ—¥æœŸ/æ™‚é–“ï¼ˆåšæˆ timestampï¼Œä¹‹å¾Œç”¨æ–¼æ™‚é–“åºåˆ—åˆ‡åˆ†ï¼‰===
df = df.withColumn(
    "timestamp",
    unix_timestamp(concat_ws(" ", col("Date"), col("Time")), "yyyy-MM-dd HH:mm:ss").cast("long")
)

# === 4. åš´æ ¼æ™‚é–“åºåˆ—åˆ‡åˆ†ï¼ˆæŒ‰æ™‚é–“æ’åºã€80%è¨“ç·´/20%æ¸¬è©¦ï¼‰===
df = df.orderBy("timestamp")
total = df.count()
split_idx = int(total * 0.8)
train_data = df.limit(split_idx)
test_data = df.subtract(train_data)

# === 5. åƒ…ç”¨åŸå§‹ SAML-D æ¬„ä½ï¼ˆä¸å«ä»»ä½•åœ–ç‰¹å¾µï¼‰===
categorical_cols = [
    "Payment_currency", "Received_currency",
    "Sender_bank_location", "Receiver_bank_location", "Payment_type"
]
numeric_cols = [
    "Amount"  # åªç”¨é‡‘é¡
    # æ³¨æ„ï¼šä¸åŒ…å«åœ–çµæ§‹ç‰¹å¾µ
]

# === 6. é¡åˆ¥å‹è½‰æ•¸å€¼ï¼ˆStringIndexer + OneHotEncoderï¼‰===
indexers = [StringIndexer(inputCol=c, outputCol=f"{c}_idx", handleInvalid="keep") for c in categorical_cols]
encoders = [OneHotEncoder(inputCol=f"{c}_idx", outputCol=f"{c}_vec") for c in categorical_cols]
feature_cols = numeric_cols + [f"{c}_vec" for c in categorical_cols]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

# === 7. å®šç¾© ML æ¨¡å‹ ===
models = {
    "Logistic Regression": LogisticRegression(labelCol="Is_laundering", featuresCol="features"),
    "Decision Tree": DecisionTreeClassifier(labelCol="Is_laundering", featuresCol="features"),
    "Random Forest": RandomForestClassifier(labelCol="Is_laundering", featuresCol="features", numTrees=100),
    "SVM (LinearSVC)": LinearSVC(labelCol="Is_laundering", featuresCol="features")
}

# === 8. è©•ä¼°æŒ‡æ¨™ ===
binary_eval = BinaryClassificationEvaluator(labelCol="Is_laundering", metricName="areaUnderROC")
precision_eval = MulticlassClassificationEvaluator(labelCol="Is_laundering", predictionCol="prediction", metricName="precisionByLabel")
recall_eval = MulticlassClassificationEvaluator(labelCol="Is_laundering", predictionCol="prediction", metricName="recallByLabel")
f1_eval = MulticlassClassificationEvaluator(labelCol="Is_laundering", predictionCol="prediction", metricName="f1")

# === 9. è¨“ç·´æ¯å€‹æ¨¡å‹ï¼Œä¸¦å°å‡ºè©•ä¼°æŒ‡æ¨™èˆ‡ç‰¹å¾µé‡è¦æ€§ ===
for name, clf in models.items():
    print(f"ğŸ”¹ {name}")
    start = time.time()
    pipeline = Pipeline(stages=indexers + encoders + [assembler, clf])
    model = pipeline.fit(train_data)
    predictions = model.transform(test_data)
    elapsed = time.time() - start

    auc = binary_eval.evaluate(predictions)
    precision = precision_eval.evaluate(predictions)
    recall = recall_eval.evaluate(predictions)
    f1 = f1_eval.evaluate(predictions)

    print(f"   ğŸ•’ è¨“ç·´+é æ¸¬æ™‚é–“ï¼š{elapsed:.2f} ç§’")
    print(f"   ğŸ“ˆ AUC          ï¼š{auc:.4f}")
    print(f"   ğŸ¯ Precision    ï¼š{precision:.4f}")
    print(f"   ğŸ¯ Recall       ï¼š{recall:.4f}")
    print(f"   ğŸ§® F1 Score     ï¼š{f1:.4f}\n")

    # === ç‰¹å¾µé‡è¦æ€§ï¼ˆåªæœ‰DT/RFæœ‰ .featureImportancesï¼‰===
    if name in ["Decision Tree", "Random Forest"]:
        importances = model.stages[-1].featureImportances
        print("   ğŸ”¬ Feature Importances:")
        feature_names = numeric_cols.copy()
        for c in categorical_cols:
            ohe_size = 20
            feature_names.extend([f"{c}_vec_{i}" for i in range(ohe_size)])
        imp_list = list(importances)
        sorted_features = sorted(zip(feature_names, imp_list), key=lambda x: -x[1])[:10]
        for fname, score in sorted_features:
            print(f"      {fname:<25} {score:.4f}")
        print()
    elif name == "Logistic Regression":
        coefs = model.stages[-1].coefficients.toArray()
        top_idx = abs(coefs).argsort()[-10:][::-1]
        for idx in top_idx:
            print(f"      feature_{idx:<2} abs(coef): {abs(coefs[idx]):.4f}")
        print()
    elif name == "SVM (LinearSVC)":
        coefs = model.stages[-1].coefficients.toArray()
        top_idx = abs(coefs).argsort()[-10:][::-1]
        for idx in top_idx:
            print(f"      feature_{idx:<2} abs(coef): {abs(coefs[idx]):.4f}")
        print()

print("âœ… åš´æ ¼æ™‚é–“åºåˆ—é©—è­‰ï¼ˆåªç”¨åŸå§‹ç‰¹å¾µï¼‰ML pipeline åŸ·è¡Œå®Œç•¢ï¼")
